\documentclass{article}
\usepackage[letterpaper]{geometry}
\usepackage{setspace}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{hyperref}


% text layout issues
\setlength{\textwidth}{5.25in}
\setlength{\oddsidemargin}{0.625in}
\setlength{\textheight}{9.00in}
\setlength{\topmargin}{-0.50in}
\setstretch{1.08}
\sloppy\sloppypar\raggedbottom
\frenchspacing

% math macros
\newcommand{\dd}{\mathrm{d}}
\newcommand{\semic}{\,;\,}
\newcommand{\given}{\,|\,}

\title{\bfseries%
Group averaging and statistical marginalization:
Is there a connection?}
\author{Hogg \& Clark for Villar}
\date{August 2025}

\begin{document}

\maketitle

In machine-learning contexts, there is a known trick---known but not (to my knowledge) widely used---that a non-equivariant machine-learning method can be made equivariant (and made more accurate) by \emph{group averaging}.
That is, if a trained function $f(x\semic W)$, with weights $W$ found by training on training data, was trained to predict labels $y$, and if the true relationship between $x$ and $y$ is equivariant to a group $G$, then the group-averaged function $\bar{f}_G(x\semic W)$ will (in many cases) outperform $f(x\semic W)$ in predictive accuracy.
The group averaging looks like
\begin{align}
    \bar{f}_G(x\semic W) &= \frac{1}{|G|}\,\sum_{g\in G} g_y^{-1}\cdot f(g_x\cdot x\semic W) ~,
\end{align}
where the outer $g_y^{-1}\cdot$ operator is the action of the (inverse) group operator $g^{-1}$ acting on the output $y$-space, and the inner $g_x\cdot$ is the action of the group operator $g$ acting on the input space.
In the case of group-invariant functions (which are of most importance here), the outer $g_y^{-1}\cdot$ is just the identity.

The conditions under which the group-averaged function $\bar{f}_G(x\semic W)$ provably makes better predictions for the labels $y$ are\ldots SOMETHING.
But in practice it often seems to help.

In inference contexts, \emph{marginalization} is used to remove nuisance parameters that are not of interest to the final inferences.
If a likelihood function $p(y\given\theta,\alpha)$ depends on parameters of interest $\theta$ and nuisance parameters $\alpha$, and if the investigator has, as part of their model (or, if Bayesian, part of their beliefs about the world) a prior pdf $p(\alpha)$ for the nuisance parameters $\alpha$, then the likelihood can be marginalized such that it only depends on $\theta$:
\begin{align}
    p(y\given\theta) &= \int p(y\given\theta,\alpha)\,p(\alpha)\,\dd\alpha ~,
\end{align}
where the integral (implicitly) goes over the whole domain of the nuisances $\alpha$, and (implicitly) all pdfs are properly normalized.
This marginalization can be done by frequentists if the distribution over $\alpha$ is part of their model.
This marginalization \emph{must} be done by Bayesians if they want to make their best possible probabilistic predictions for $\theta$.
In the end, Bayesian predictions are made by multiplying the likelihood by priors and renormalizing:
\begin{align}
    p(\theta\given y) = \frac{1}{Z}\,p(\theta)\int p(y\given\theta,\alpha)\,p(\alpha)\,\dd\alpha ~,
\end{align}
where $p(\theta\given y)$ is a pdf representing posterior beliefs about the parameters of interest $\theta$ (given the data $y$),
$p(\theta)$ is a pdf representing prior beliefs,
and $Z$ is a normalization constant to make the LHS a pdf.
There are Cox theorems (HOGG CITE) that prove that Bayesian reasoners win bets against all other reasoners, in contexts in which the Bayesians bet against the other reasoners, so there are conditions in which this marginalization will provably lead to better predictions than any other operation.
Also in practice this kind of marginalization is useful and works well.

Now let's get more specific.
The world generates a true label $\tilde{y}$ from features $x$ in such a way that it is \emph{invariant} to some latent angle or set of latent angles $\alpha$.
This \ldots

\paragraph{Can we run this in practice?}
Well... Running the marginalization is quite expensive. This is because we usually can't integrate it directly, so we usually MCMC it:
\begin{align}
	\int p(y | \theta, \alpha) p(\alpha) d\alpha & = \mathbb E_{\alpha \sim p(\alpha)}[p(y|\theta, \alpha)] \\
	& \approx \frac{1}{N} \sum_{i=1}^N p(y|\theta, \alpha_i)
\end{align}
So if we were doing this inference on an incredibly large dataset (for example, all of the Gaia data), you're out of luck (in that particular example, you'll burn 20\% of Flatiron's yearly compute budget). 

After looking at the equation above for sometime, you might ask, "What if I knew the `right' set of $\{\alpha_i\}_i$, that me closest to the `best' answer?" You can think of the discrete set $\{\alpha_i\}_i$ as a measure corresponding to a discrete distribution. And the measurement of best can be what recovers the $k$'th moments of a sufficient statistic. After some googling, you realize this is the study of...

\paragraph{$k$-Designs}
Let's start with a heuristic definition, and then talk about real examples. Consider a function $f_k$ which is a polynomial to the power $k$. We say probability distribution $\nu$ forms a \textbf{$k$-design} with respect to the target distribution $\mu$ if
\begin{align}
	\mathbb E_{x \sim \nu}[f_k(x)] = \mathbb E_{x \sim \mu}[f_k(x)]
\end{align}
So a couple comments:
\begin{itemize}
	\item A natural interpretation is the $k$-design preserves the $k$'th moment.
	\item Say $\mu$ is a continuous distribution, $\nu$ need not be continuous, it can be discrete! This means instead of sampling, we can evaluate the function on a finite set of points-- very computationally quick! 
	\item $\nu$ need not be unique.
\end{itemize}
You can see finding $k$-designs saves us time \& has an implicit bound on how we diverge in our sufficient statistic. In this statistics context, it seems that we'll only need a $2$-design.

\emph{Example (Spherical Designs \cite{colbourn2006handbook, delsarte1977spherical}):} Let $p_k : \mathcal S(\mathbb R^d) \to \mathbb R$ be a polynomial in $d$ variables, with all terms homogenous in degree at most $k$. A set $X = \{x : x \in \mathcal S(\mathbb R^d)\}$ is a spherical $k$-design if 
\begin{align}
	\frac{1}{|X|} \sum_{x \in X}p_k(x) = \int_{\mathcal S(\mathbb R^d)} p_k(u) d\mu(u)
\end{align} holds for all possible $p_t$, where $d\mu$ is the uniform, normalized spherical measure. A spherical $k$-design is also a $k'$-design for all $k' < k$. These designs are very pretty because the elements of their event space are vertices the certain platonic solids (for example: the regular ($k$ + 1)-gon in $\mathcal S^2$ is a spherical $k$-design).


\paragraph{Finding $k$-Designs}
\begin{itemize}
	\item Dries: you can find them via optimal control (it's not easy but that is an algorithmic way of finding them).
	\item  Clark: This reminds me of rejection sampling (like construct a set $A = \{\alpha_i\}_i$, and ask which element $\alpha \in A$ should I s.t. the error of your sufficient statistics changes the least, do this over a training dataset and test set.).
\end{itemize}



% References
\newpage
\bibliographystyle{plain}
\bibliography{refs}

\end{document}
